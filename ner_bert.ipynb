{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with Bert\n",
    "\n",
    "Source: https://medium.com/@andrewmarmon/fine-tuned-named-entity-recognition-with-hugging-face-bert-d51d4cb3d7b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O','B-MISC','I-MISC','B-PER','I-PER','B-ORG','I-ORG','B-LOC','I-LOC']\n",
    "label_encoding_dict = {'I-PRG': 2,'I-I-MISC': 2, 'I-OR': 6,  'I-': 0, 'VMISC': 0, # line covers ann=tation erros in train file \n",
    "                       'O': 0, 'B-MISC': 1, 'I-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8,}\n",
    "\n",
    "task = \"ner\" \n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tokens_and_ner_tags(directory):\n",
    "    return pd.concat([get_tokens_and_ner_tags(os.path.join(directory, filename)) for filename in os.listdir(directory)]).reset_index().drop('index', axis=1)\n",
    "    \n",
    "def get_tokens_and_ner_tags(filename):\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        split_list = [list(y) for x, y in itertools.groupby(lines, lambda z: z == '\\n') if not x]\n",
    "        tokens = [[x.split('\\t')[0] for x in y] for y in split_list]\n",
    "        entities = [[x.split('\\t')[1][:-1] for x in y] for y in split_list] \n",
    "    return pd.DataFrame({'tokens': tokens, 'ner_tags': entities})\n",
    "  \n",
    "def get_un_token_dataset(train_directory, test_directory):\n",
    "    train_df = get_all_tokens_and_ner_tags(train_directory)\n",
    "    test_df = get_all_tokens_and_ner_tags(test_directory)\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    return (train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    label_all_tokens = True\n",
    "    tokenized_inputs = tokenizer(list(examples[\"tokens\"]), truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif label[word_idx] == '0':\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] \n",
    "                         for (p, l) in zip(prediction, label) \n",
    "                         if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"], \n",
    "            \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \n",
    "            \"accuracy\": results[\"overall_accuracy\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = get_un_token_dataset('../../data/UN-named-entity-recognition/tagged-training/', \n",
    "                                                   '../../data/UN-named-entity-recognition/tagged-test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.76ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 19.07ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_7980/951226195.py:14: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/erikt/projects/rel20/venv3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3657\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 687\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='687' max='687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [687/687 13:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.052608</td>\n",
       "      <td>0.789836</td>\n",
       "      <td>0.850255</td>\n",
       "      <td>0.818933</td>\n",
       "      <td>0.982015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.049980</td>\n",
       "      <td>0.830493</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.844313</td>\n",
       "      <td>0.984916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.057054</td>\n",
       "      <td>0.828901</td>\n",
       "      <td>0.866945</td>\n",
       "      <td>0.847496</td>\n",
       "      <td>0.985039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2074\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2074\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test-ner/checkpoint-500\n",
      "Configuration saved in test-ner/checkpoint-500/config.json\n",
      "Model weights saved in test-ner/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-ner/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-ner/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2074\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2074\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to un-ner.model\n",
      "Configuration saved in un-ner.model/config.json\n",
      "Model weights saved in un-ner.model/pytorch_model.bin\n",
      "tokenizer config file saved in un-ner.model/tokenizer_config.json\n",
      "Special tokens file saved in un-ner.model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_tokenized_datasets,\n",
    "    eval_dataset=test_tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model('un-ner.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./un-ner.model/added_tokens.json. We won't load it.\n",
      "loading file ./un-ner.model/vocab.txt\n",
      "loading file ./un-ner.model/tokenizer.json\n",
      "loading file None\n",
      "loading file ./un-ner.model/special_tokens_map.json\n",
      "loading file ./un-ner.model/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 103])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./un-ner.model/')\n",
    "\n",
    "paragraph = '''\n",
    "    Before proceeding further, I should like to inform members that action on draft \n",
    "    resolution iv, entitled situation of human rights of Rohingya Muslims and other \n",
    "    minorities in Myanmar is postponed to a later date to allow time for the review \n",
    "    of its programme budget implications by the fifth committee. The assembly will \n",
    "    take action on draft resolution iv as soon as the report of the fifth committee \n",
    "    on the programme budget implications is available. I now give the floor to \n",
    "    delegations wishing to deliver explanations of vote or position before voting or \n",
    "    adoption.'''\n",
    "tokens = tokenizer(paragraph)\n",
    "torch.tensor(tokens['input_ids']).unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./un-ner.model/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./un-ner.model/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./un-ner.model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at ./un-ner.model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('./un-ner.model/', num_labels=len(label_list))\n",
    "predictions = model.forward(input_ids=torch.tensor(tokens['input_ids']).unsqueeze(0), attention_mask=torch.tensor(tokens['attention_mask']).unsqueeze(0))\n",
    "predictions = torch.argmax(predictions.logits.squeeze(), axis=1)\n",
    "predictions = [label_list[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer.batch_decode(tokens['input_ids'])\n",
    "pd.DataFrame({'ner': predictions, 'words': words}).to_csv('un_ner.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert large NER\n",
    "\n",
    "Source: https://huggingface.co/dslim/bert-large-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/dslim/bert-large-NER/resolve/main/config.json from cache at /home/erikt/.cache/huggingface/transformers/5a0d81781c7e4fc475ede69166dd515e7c5b54df30bae5185d675057f9352ff8.70a92b0531aa1dfc5ed255c8492db74ef24c17365b0fe86f1fc3ef4e3f028cd2\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dslim/bert-large-NER\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/dslim/bert-large-NER/resolve/main/vocab.txt from cache at /home/erikt/.cache/huggingface/transformers/2084ed8041ff7b2ff2eaa172eb23ca0cf6e468dbe366538873b0085d3750b03f.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/dslim/bert-large-NER/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/dslim/bert-large-NER/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dslim/bert-large-NER/resolve/main/special_tokens_map.json from cache at /home/erikt/.cache/huggingface/transformers/621119842bc8dceeb517def16906c5678b5eb8e02163dd6e69e0a31f7e3246ad.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/dslim/bert-large-NER/resolve/main/tokenizer_config.json from cache at /home/erikt/.cache/huggingface/transformers/5dab641aaa22c41ddae30ae031c0c6f4f1df6d2be0f6cbc32ae73dce76d42344.f19de0c372e9b00104464e8b09d5fbbdd67565d0e0af78462fb22d8f5d2c1fe1\n",
      "loading configuration file https://huggingface.co/dslim/bert-large-NER/resolve/main/config.json from cache at /home/erikt/.cache/huggingface/transformers/5a0d81781c7e4fc475ede69166dd515e7c5b54df30bae5185d675057f9352ff8.70a92b0531aa1dfc5ed255c8492db74ef24c17365b0fe86f1fc3ef4e3f028cd2\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dslim/bert-large-NER\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dslim/bert-large-NER/resolve/main/config.json from cache at /home/erikt/.cache/huggingface/transformers/5a0d81781c7e4fc475ede69166dd515e7c5b54df30bae5185d675057f9352ff8.70a92b0531aa1dfc5ed255c8492db74ef24c17365b0fe86f1fc3ef4e3f028cd2\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dslim/bert-large-NER\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dslim/bert-large-NER/resolve/main/config.json from cache at /home/erikt/.cache/huggingface/transformers/5a0d81781c7e4fc475ede69166dd515e7c5b54df30bae5185d675057f9352ff8.70a92b0531aa1dfc5ed255c8492db74ef24c17365b0fe86f1fc3ef4e3f028cd2\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dslim/bert-large-NER\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"I-MISC\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-PER\",\n",
      "    \"5\": \"B-ORG\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"B-LOC\",\n",
      "    \"8\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 7,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 5,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 8,\n",
      "    \"I-MISC\": 2,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/dslim/bert-large-NER/resolve/main/pytorch_model.bin from cache at /home/erikt/.cache/huggingface/transformers/e7b8a29f57239a195c3a6f9accba4b7bc71411c3028c690ff4425c3269a27fae.a103085ff735e3584e329b8ffd0c27d439f6e6e9993d797559212e645c5b7db6\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at dslim/bert-large-NER.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9971501, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.9986046, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"SOCCER - GERMAN FIRST DIVISION SUMMARIES. BONN 1996-12-06 Summaries of matches played in the German first division on Friday : Bochum 2 (Stickroth 30th pen, Wosz 89th) Bayer Leverkusen 2 (Kirsten 18th, Ramelow 56th). Halftime 1-1. Attendance : 24,602 Werder Bremen 1 (Bode 85th) 1860 Munich 1 (Bormirow 8th). Halftime 0-1. Attendance 33,000 Karlsruhe 3 (Reich 29th, Carl 44th, Dundee 69th) Freiburg 0. Halftime 2-0. Attendance 33,000 Schalke 2 (Mulder 2nd and 27th) Hansa Rostock 0. Halftime 2-0. Attendance 29,300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-MISC', 'score': 0.99924064, 'index': 6, 'word': 'GE', 'start': 9, 'end': 11}, {'entity': 'I-MISC', 'score': 0.8793608, 'index': 7, 'word': '##R', 'start': 11, 'end': 12}, {'entity': 'I-MISC', 'score': 0.884922, 'index': 8, 'word': '##MA', 'start': 12, 'end': 14}, {'entity': 'I-MISC', 'score': 0.9773442, 'index': 9, 'word': '##N', 'start': 14, 'end': 15}, {'entity': 'B-MISC', 'score': 0.9992536, 'index': 40, 'word': 'German', 'start': 93, 'end': 99}, {'entity': 'B-ORG', 'score': 0.9987205, 'index': 46, 'word': 'Bo', 'start': 127, 'end': 129}, {'entity': 'I-ORG', 'score': 0.99158394, 'index': 47, 'word': '##chu', 'start': 129, 'end': 132}, {'entity': 'I-ORG', 'score': 0.9947837, 'index': 48, 'word': '##m', 'start': 132, 'end': 133}, {'entity': 'B-PER', 'score': 0.998579, 'index': 51, 'word': 'Stick', 'start': 137, 'end': 142}, {'entity': 'B-PER', 'score': 0.44963565, 'index': 52, 'word': '##roth', 'start': 142, 'end': 146}, {'entity': 'B-PER', 'score': 0.9985172, 'index': 56, 'word': 'W', 'start': 157, 'end': 158}, {'entity': 'I-PER', 'score': 0.9197838, 'index': 57, 'word': '##os', 'start': 158, 'end': 160}, {'entity': 'I-PER', 'score': 0.739916, 'index': 58, 'word': '##z', 'start': 160, 'end': 161}, {'entity': 'B-ORG', 'score': 0.99935025, 'index': 62, 'word': 'Bay', 'start': 168, 'end': 171}, {'entity': 'I-ORG', 'score': 0.9981505, 'index': 63, 'word': '##er', 'start': 171, 'end': 173}, {'entity': 'I-ORG', 'score': 0.99845874, 'index': 64, 'word': 'Lev', 'start': 174, 'end': 177}, {'entity': 'I-ORG', 'score': 0.9981659, 'index': 65, 'word': '##er', 'start': 177, 'end': 179}, {'entity': 'I-ORG', 'score': 0.9964371, 'index': 66, 'word': '##kus', 'start': 179, 'end': 182}, {'entity': 'I-ORG', 'score': 0.99873143, 'index': 67, 'word': '##en', 'start': 182, 'end': 184}, {'entity': 'B-PER', 'score': 0.99836916, 'index': 70, 'word': 'Ki', 'start': 188, 'end': 190}, {'entity': 'B-PER', 'score': 0.98304445, 'index': 71, 'word': '##rsten', 'start': 190, 'end': 195}, {'entity': 'B-PER', 'score': 0.9985753, 'index': 74, 'word': 'Ram', 'start': 202, 'end': 205}, {'entity': 'I-PER', 'score': 0.9433469, 'index': 75, 'word': '##elo', 'start': 205, 'end': 208}, {'entity': 'I-PER', 'score': 0.59934485, 'index': 76, 'word': '##w', 'start': 208, 'end': 209}, {'entity': 'B-ORG', 'score': 0.99929786, 'index': 93, 'word': 'We', 'start': 251, 'end': 253}, {'entity': 'I-ORG', 'score': 0.99351996, 'index': 94, 'word': '##rde', 'start': 253, 'end': 256}, {'entity': 'I-ORG', 'score': 0.99213785, 'index': 95, 'word': '##r', 'start': 256, 'end': 257}, {'entity': 'I-ORG', 'score': 0.99871624, 'index': 96, 'word': 'Bremen', 'start': 258, 'end': 264}, {'entity': 'B-PER', 'score': 0.9979316, 'index': 99, 'word': 'Bo', 'start': 268, 'end': 270}, {'entity': 'I-PER', 'score': 0.59899634, 'index': 100, 'word': '##de', 'start': 270, 'end': 272}, {'entity': 'B-ORG', 'score': 0.99926776, 'index': 104, 'word': '1860', 'start': 279, 'end': 283}, {'entity': 'I-ORG', 'score': 0.99897707, 'index': 105, 'word': 'Munich', 'start': 284, 'end': 290}, {'entity': 'B-PER', 'score': 0.9982256, 'index': 108, 'word': 'Bo', 'start': 294, 'end': 296}, {'entity': 'B-PER', 'score': 0.64868224, 'index': 110, 'word': '##iro', 'start': 298, 'end': 301}, {'entity': 'I-PER', 'score': 0.65515053, 'index': 111, 'word': '##w', 'start': 301, 'end': 302}, {'entity': 'B-ORG', 'score': 0.9992824, 'index': 125, 'word': 'Karl', 'start': 341, 'end': 345}, {'entity': 'I-ORG', 'score': 0.9980318, 'index': 126, 'word': '##s', 'start': 345, 'end': 346}, {'entity': 'I-ORG', 'score': 0.99667144, 'index': 127, 'word': '##ru', 'start': 346, 'end': 348}, {'entity': 'I-ORG', 'score': 0.9923685, 'index': 128, 'word': '##he', 'start': 348, 'end': 350}, {'entity': 'B-PER', 'score': 0.9988121, 'index': 131, 'word': 'Reich', 'start': 354, 'end': 359}, {'entity': 'B-PER', 'score': 0.9981529, 'index': 134, 'word': 'Carl', 'start': 366, 'end': 370}, {'entity': 'B-PER', 'score': 0.99630994, 'index': 138, 'word': 'Dundee', 'start': 377, 'end': 383}, {'entity': 'B-ORG', 'score': 0.9992298, 'index': 142, 'word': 'Freiburg', 'start': 390, 'end': 398}, {'entity': 'B-ORG', 'score': 0.9993808, 'index': 155, 'word': 'Sc', 'start': 434, 'end': 436}, {'entity': 'I-ORG', 'score': 0.9970982, 'index': 156, 'word': '##hal', 'start': 436, 'end': 439}, {'entity': 'I-ORG', 'score': 0.9967765, 'index': 157, 'word': '##ke', 'start': 439, 'end': 441}, {'entity': 'B-PER', 'score': 0.99819237, 'index': 160, 'word': 'Mu', 'start': 445, 'end': 447}, {'entity': 'B-PER', 'score': 0.55266804, 'index': 161, 'word': '##lder', 'start': 447, 'end': 451}, {'entity': 'B-ORG', 'score': 0.9992756, 'index': 166, 'word': 'Hans', 'start': 466, 'end': 470}, {'entity': 'I-ORG', 'score': 0.9984034, 'index': 167, 'word': '##a', 'start': 470, 'end': 471}, {'entity': 'I-ORG', 'score': 0.99900514, 'index': 168, 'word': 'R', 'start': 472, 'end': 473}, {'entity': 'I-ORG', 'score': 0.9982064, 'index': 169, 'word': '##ost', 'start': 473, 'end': 476}, {'entity': 'I-ORG', 'score': 0.99908614, 'index': 170, 'word': '##ock', 'start': 476, 'end': 479}]\n"
     ]
    }
   ],
   "source": [
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_entities(ner_results):\n",
    "    ner_results_out = []\n",
    "    i = 0\n",
    "    while i < len(ner_results)-1:\n",
    "        last_end = ner_results[i][\"end\"]\n",
    "        ner_results_out.append(dict(ner_results[i]))\n",
    "        j = 1\n",
    "        while i + j < len(ner_results) and (ner_results[i+j][\"start\"] == last_end or\n",
    "                                            (ner_results[i+j][\"start\"] == last_end + 1 and \n",
    "                                             re.search(\"^I\", ner_results[i+j][\"entity\"]) and\n",
    "                                             re.sub(\"^..\", \"\", ner_results[i+j][\"entity\"]) == re.sub(\"^..\", \"\", ner_results[i][\"entity\"]))):\n",
    "            if ner_results[i+j][\"start\"] == last_end:\n",
    "                ner_results_out[-1][\"word\"] += re.sub(\"^##\", \"\", ner_results[i+j][\"word\"])\n",
    "            else:\n",
    "                ner_results_out[-1][\"word\"] += \" \" + ner_results[i+j][\"word\"]\n",
    "            ner_results_out[-1][\"end\"] = ner_results[i+j][\"end\"]\n",
    "            last_end = ner_results[i+j][\"end\"]\n",
    "            j += 1\n",
    "        i += j\n",
    "    return ner_results_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-MISC',\n",
       "  'score': 0.99924064,\n",
       "  'index': 6,\n",
       "  'word': 'GERMAN',\n",
       "  'start': 9,\n",
       "  'end': 15},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9992536,\n",
       "  'index': 40,\n",
       "  'word': 'German',\n",
       "  'start': 93,\n",
       "  'end': 99},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.9987205,\n",
       "  'index': 46,\n",
       "  'word': 'Bochum',\n",
       "  'start': 127,\n",
       "  'end': 133},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.998579,\n",
       "  'index': 51,\n",
       "  'word': 'Stickroth',\n",
       "  'start': 137,\n",
       "  'end': 146},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9985172,\n",
       "  'index': 56,\n",
       "  'word': 'Wosz',\n",
       "  'start': 157,\n",
       "  'end': 161},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.99935025,\n",
       "  'index': 62,\n",
       "  'word': 'Bayer Leverkusen',\n",
       "  'start': 168,\n",
       "  'end': 184},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99836916,\n",
       "  'index': 70,\n",
       "  'word': 'Kirsten',\n",
       "  'start': 188,\n",
       "  'end': 195},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9985753,\n",
       "  'index': 74,\n",
       "  'word': 'Ramelow',\n",
       "  'start': 202,\n",
       "  'end': 209},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.99929786,\n",
       "  'index': 93,\n",
       "  'word': 'Werder Bremen',\n",
       "  'start': 251,\n",
       "  'end': 264},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9979316,\n",
       "  'index': 99,\n",
       "  'word': 'Bode',\n",
       "  'start': 268,\n",
       "  'end': 272},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.99926776,\n",
       "  'index': 104,\n",
       "  'word': '1860 Munich',\n",
       "  'start': 279,\n",
       "  'end': 290},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9982256,\n",
       "  'index': 108,\n",
       "  'word': 'Bo',\n",
       "  'start': 294,\n",
       "  'end': 296},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.64868224,\n",
       "  'index': 110,\n",
       "  'word': '##irow',\n",
       "  'start': 298,\n",
       "  'end': 302},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.9992824,\n",
       "  'index': 125,\n",
       "  'word': 'Karlsruhe',\n",
       "  'start': 341,\n",
       "  'end': 350},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9988121,\n",
       "  'index': 131,\n",
       "  'word': 'Reich',\n",
       "  'start': 354,\n",
       "  'end': 359},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9981529,\n",
       "  'index': 134,\n",
       "  'word': 'Carl',\n",
       "  'start': 366,\n",
       "  'end': 370},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99630994,\n",
       "  'index': 138,\n",
       "  'word': 'Dundee',\n",
       "  'start': 377,\n",
       "  'end': 383},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.9992298,\n",
       "  'index': 142,\n",
       "  'word': 'Freiburg',\n",
       "  'start': 390,\n",
       "  'end': 398},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.9993808,\n",
       "  'index': 155,\n",
       "  'word': 'Schalke',\n",
       "  'start': 434,\n",
       "  'end': 441},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99819237,\n",
       "  'index': 160,\n",
       "  'word': 'Mulder',\n",
       "  'start': 445,\n",
       "  'end': 451},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.9992756,\n",
       "  'index': 166,\n",
       "  'word': 'Hansa Rostock',\n",
       "  'start': 466,\n",
       "  'end': 479}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_entities(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
